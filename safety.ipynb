{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained MobileNet SSD model and the class labels\n",
    "net = cv2.dnn.readNetFromCaffe(\"C:\\\\Users\\\\vashi\\\\OneDrive\\\\Desktop\\\\women\\\\deploy.prototxt\", \"C:\\\\Users\\\\vashi\\\\OneDrive\\\\Desktop\\\\women\\\\mobilenet_iter_73000 (2).caffemodel\")\n",
    "classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
    "           \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\",\n",
    "           \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# Start capturing video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame for object detection\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Loop over the detections\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            if classes[idx] == \"person\":\n",
    "                # Draw a bounding box around the detected person\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                label = f\"Person: {confidence:.2f}\"\n",
    "                cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GENDER CLASSIFICATION MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def faceBox(faceNet, frame):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (227, 227), [104, 117, 123], swapRB=False)\n",
    "    faceNet.setInput(blob)\n",
    "    detection = faceNet.forward()\n",
    "    bboxs = []\n",
    "    for i in range(detection.shape[2]):\n",
    "        confidence = detection[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            x1 = int(detection[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detection[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detection[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detection[0, 0, i, 6] * frameHeight)\n",
    "            bboxs.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    return frame, bboxs\n",
    "\n",
    "facePhoto = \"opencv_face_detector.pbtxt\"\n",
    "face_Model = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "faceNet = cv2.dnn.readNet(face_Model, facePhoto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = [ 'Male','Female']\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "padding=20\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame, bboxs = faceBox(faceNet, frame)\n",
    "    for bbox in bboxs:\n",
    "       # face = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        face=frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding,frame.shape[1]-1)]\n",
    "        if face.size == 0:\n",
    "            continue\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPrediction = genderNet.forward()\n",
    "        gender = genderList[genderPrediction[0].argmax()]\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePred = ageNet.forward()\n",
    "        age = ageList[agePred[0].argmax()]\n",
    "\n",
    "        label = \"{}, {}\".format(gender, age)\n",
    "        cv2.rectangle(frame,(bbox[0],bbox[1]-30),(bbox[2],bbox[1]),(0,255,0),-1)\n",
    "        cv2.putText(frame, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2,cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Age_gender\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ANOMALY DETECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def classify_gender(face):\n",
    "    # Implement gender classification logic using a pre-trained model\n",
    "    # For demonstration, we will use a dummy implementation\n",
    "    # Replace this with actual model inference code\n",
    "    return \"Female\"  # or \"Male\", depending on your actual implementation\n",
    "\n",
    "def is_lone_woman_detected(gender_counts):\n",
    "    current_hour = datetime.now().hour\n",
    "    if current_hour >= 20 or current_hour <= 6:  # Consider night hours\n",
    "        if gender_counts[\"Female\"] == 1 and gender_counts[\"Male\"] == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Load models and initialize\n",
    "video = cv2.VideoCapture(0)  # Use 'video' as the capture variable\n",
    "\n",
    "# Ensure these paths are correct and the files exist\n",
    "face_prototxt = \"C:\\\\Users\\\\vashi\\\\OneDrive\\\\Desktop\\\\women\\\\deploy.prototxt\"\n",
    "face_model = \"C:\\\\Users\\\\vashi\\\\OneDrive\\\\Desktop\\\\women\\\\mobilenet_iter_73000 (2).caffemodel\"\n",
    "\n",
    "try:\n",
    "    net = cv2.dnn.readNetFromCaffe(face_prototxt, face_model)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \n",
    "           \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\",\n",
    "           \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    try:\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during detection: {e}\")\n",
    "        continue\n",
    "\n",
    "    gender_counts = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            if classes[idx] == \"person\":\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                face = frame[startY:endY, startX:endX]\n",
    "\n",
    "                gender = classify_gender(face)\n",
    "                if gender in gender_counts:\n",
    "                    gender_counts[gender] += 1\n",
    "                else:\n",
    "                    print(f\"Unexpected gender value: {gender}\")\n",
    "\n",
    "                label = f\"{gender} ({confidence:.2f})\"\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2,cv2.LINE_AA)\n",
    "\n",
    "    if is_lone_woman_detected(gender_counts):\n",
    "        cv2.putText(frame, \"ALERT: Lone Woman Detected!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full complete model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def classify_gender(face, genderNet, genderList):\n",
    "    if face.size == 0:\n",
    "        return \"Unknown\"\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPrediction = genderNet.forward()\n",
    "    return genderList[genderPrediction[0].argmax()]\n",
    "\n",
    "def classify_age(face, ageNet, ageList):\n",
    "    if face.size == 0:\n",
    "        return \"Unknown\"\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    ageNet.setInput(blob)\n",
    "    agePred = ageNet.forward()\n",
    "    return ageList[agePred[0].argmax()]\n",
    "\n",
    "def is_lone_woman_detected(gender_counts):\n",
    "    current_hour = datetime.now().hour\n",
    "    if current_hour >= 20 or current_hour <= 6:  # Consider night hours\n",
    "        if gender_counts[\"Female\"] == 1 and gender_counts[\"Male\"] == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def faceBox(faceNet, frame):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (227, 227), [104, 117, 123], swapRB=False)\n",
    "    faceNet.setInput(blob)\n",
    "    detection = faceNet.forward()\n",
    "    bboxs = []\n",
    "    for i in range(detection.shape[2]):\n",
    "        confidence = detection[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            x1 = int(detection[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detection[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detection[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detection[0, 0, i, 6] * frameHeight)\n",
    "            bboxs.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    return frame, bboxs\n",
    "\n",
    "# Load models and initialize\n",
    "facePhoto = \"opencv_face_detector.pbtxt\"\n",
    "face_Model = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "faceNet = cv2.dnn.readNet(face_Model, facePhoto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "padding = 20\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame, bboxs = faceBox(faceNet, frame)\n",
    "    gender_counts = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "    for bbox in bboxs:\n",
    "        face = frame[max(0, bbox[1] - padding):min(bbox[3] + padding, frame.shape[0] - 1),\n",
    "                     max(0, bbox[0] - padding):min(bbox[2] + padding, frame.shape[1] - 1)]\n",
    "        \n",
    "        gender = classify_gender(face, genderNet, genderList)\n",
    "        age = classify_age(face, ageNet, ageList)\n",
    "\n",
    "        if gender != \"Unknown\":\n",
    "            gender_counts[gender] += 1\n",
    "\n",
    "        label = \"{}, {}\".format(gender, age)\n",
    "        cv2.rectangle(frame, (bbox[0], bbox[1] - 30), (bbox[2], bbox[1]), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    if is_lone_woman_detected(gender_counts):\n",
    "        cv2.putText(frame, \"ALERT: Lone Woman Detected!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Age_gender\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load models for face detection, age, and gender classification\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "gestureModel = \"gesture_recognition_model.h5\"  # Placeholder for gesture recognition model\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "gestureNet = None  # Load gesture recognition model if available\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Function to detect faces\n",
    "def faceBox(faceNet, frame):\n",
    "    frameHeight, frameWidth = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], swapRB=False)\n",
    "    faceNet.setInput(blob)\n",
    "    detection = faceNet.forward()\n",
    "    bboxs = []\n",
    "    for i in range(detection.shape[2]):\n",
    "        confidence = detection[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            x1 = int(detection[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detection[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detection[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detection[0, 0, i, 6] * frameHeight)\n",
    "            bboxs.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    return frame, bboxs\n",
    "\n",
    "# Function to classify gender\n",
    "def classify_gender(face, genderNet, genderList):\n",
    "    if face.size == 0:\n",
    "        return \"Unknown\"\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPred = genderNet.forward()\n",
    "    return genderList[genderPred[0].argmax()]\n",
    "\n",
    "# Function to detect gestures (Placeholder)\n",
    "def detect_gesture(frame, gestureNet):\n",
    "    # Placeholder function for gesture detection\n",
    "    # Implement your gesture detection model here\n",
    "    # Example: Return 'Suspicious' if a threatening gesture is detected\n",
    "    return \"Normal\"  # Default to 'Normal' if no gesture model is available\n",
    "\n",
    "# Function to check for lone woman\n",
    "def is_lone_woman_detected(gender_counts):\n",
    "    current_hour = datetime.now().hour\n",
    "    if (current_hour >= 20 or current_hour <= 6) and gender_counts['Female'] == 1 and gender_counts['Male'] == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check for anomalies (Placeholder)\n",
    "def detect_anomalies(frame, gender_counts):\n",
    "    # Placeholder for advanced anomaly detection\n",
    "    # Implement your anomaly detection algorithm here\n",
    "    return False  # Return True if anomaly is detected\n",
    "\n",
    "# Real-time video capture\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "padding = 20\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame, bboxs = faceBox(faceNet, frame)\n",
    "    gender_counts = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "    for bbox in bboxs:\n",
    "        face = frame[max(0, bbox[1] - padding):min(bbox[3] + padding, frame.shape[0] - 1),\n",
    "                     max(0, bbox[0] - padding):min(bbox[2] + padding, frame.shape[1] - 1)]\n",
    "        \n",
    "        gender = classify_gender(face, genderNet, genderList)\n",
    "        if gender in gender_counts:\n",
    "            gender_counts[gender] += 1\n",
    "\n",
    "        label = f\"{gender}\"\n",
    "        cv2.rectangle(frame, (bbox[0], bbox[1] - 30), (bbox[2], bbox[1]), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    if is_lone_woman_detected(gender_counts):\n",
    "        cv2.putText(frame, \"ALERT: Lone Woman Detected!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    gesture = detect_gesture(frame, gestureNet)\n",
    "    if gesture == \"Suspicious\":\n",
    "        cv2.putText(frame, \"ALERT: Suspicious Gesture Detected!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    if detect_anomalies(frame, gender_counts):\n",
    "        cv2.putText(frame, \"ALERT: Anomaly Detected!\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Women Safety Analytics\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "Video capture released.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='women_safety.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Load models for face detection, age, gender classification, and gesture recognition\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "gestureModel = \"gesture_recognition_model.h5\"  # Replace with actual model if available\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "gestureNet = None  # Replace with actual gesture model loading if available\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Email alert configuration\n",
    "EMAIL_HOST = 'smtp.example.com'  # Replace with your SMTP server\n",
    "EMAIL_PORT = 587\n",
    "EMAIL_HOST_USER = 'your_email@example.com'\n",
    "EMAIL_HOST_PASSWORD = 'your_password'\n",
    "ALERT_RECEIVER = 'receiver_email@example.com'\n",
    "\n",
    "def send_email_alert(subject, body):\n",
    "    try:\n",
    "        msg = MIMEText(body)\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = EMAIL_HOST_USER\n",
    "        msg['To'] = ALERT_RECEIVER\n",
    "\n",
    "        with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        logging.info(f\"Email alert sent: {subject}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send email alert: {e}\")\n",
    "\n",
    "def faceBox(faceNet, frame):\n",
    "    frameHeight, frameWidth = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], swapRB=False)\n",
    "    faceNet.setInput(blob)\n",
    "    detection = faceNet.forward()\n",
    "    bboxs = []\n",
    "    for i in range(detection.shape[2]):\n",
    "        confidence = detection[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            x1 = int(detection[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detection[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detection[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detection[0, 0, i, 6] * frameHeight)\n",
    "            bboxs.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    return frame, bboxs\n",
    "\n",
    "def classify_gender(face, genderNet, genderList):\n",
    "    if face.size == 0:\n",
    "        return \"Unknown\"\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPred = genderNet.forward()\n",
    "    return genderList[genderPred[0].argmax()]\n",
    "\n",
    "def detect_gesture(frame, gestureNet):\n",
    "    if gestureNet is None:\n",
    "        return \"Normal\"\n",
    "    # Placeholder for actual gesture detection\n",
    "    return \"Normal\"\n",
    "\n",
    "def is_lone_woman_detected(gender_counts):\n",
    "    current_hour = datetime.now().hour\n",
    "    if (current_hour >= 20 or current_hour <= 6) and gender_counts['Female'] == 1 and gender_counts['Male'] == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_anomalies(frame, gender_counts):\n",
    "    # Placeholder for advanced anomaly detection\n",
    "    return False\n",
    "\n",
    "# Real-time video capture\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "padding = 20\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame.\")\n",
    "            break\n",
    "\n",
    "        frame, bboxs = faceBox(faceNet, frame)\n",
    "        gender_counts = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "        if not bboxs:\n",
    "            print(\"No faces detected.\")\n",
    "        \n",
    "        for bbox in bboxs:\n",
    "            face = frame[max(0, bbox[1] - padding):min(bbox[3] + padding, frame.shape[0] - 1),\n",
    "                         max(0, bbox[0] - padding):min(bbox[2] + padding, frame.shape[1] - 1)]\n",
    "            \n",
    "            gender = classify_gender(face, genderNet, genderList)\n",
    "            if gender in gender_counts:\n",
    "                gender_counts[gender] += 1\n",
    "\n",
    "            label = f\"{gender}\"\n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1] - 30), (bbox[2], bbox[1]), (0, 255, 0), -1)\n",
    "            cv2.putText(frame, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if is_lone_woman_detected(gender_counts):\n",
    "            alert_message = \"ALERT: Lone Woman Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_email_alert(\"Lone Woman Alert\", alert_message)\n",
    "        \n",
    "        gesture = detect_gesture(frame, gestureNet)\n",
    "        if gesture == \"Suspicious\":\n",
    "            alert_message = \"ALERT: Suspicious Gesture Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_email_alert(\"Suspicious Gesture Alert\", alert_message)\n",
    "\n",
    "        if detect_anomalies(frame, gender_counts):\n",
    "            alert_message = \"ALERT: Anomaly Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_email_alert(\"Anomaly Alert\", alert_message)\n",
    "\n",
    "        # Show the frame in OpenCV window (if applicable)\n",
    "        try:\n",
    "            cv2.imshow(\"Women Safety Analytics\", frame)\n",
    "        except cv2.error as e:\n",
    "            print(f\"OpenCV error: {e}\")\n",
    "\n",
    "        # Check for 'q' key to break the loop\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    video.release()\n",
    "    try:\n",
    "        cv2.destroyAllWindows()\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV error during cleanup: {e}\")\n",
    "    print(\"Video capture released.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with sms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "No faces detected.\n",
      "Video capture released.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='women_safety.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Twilio SMS configuration\n",
    "TWILIO_ACCOUNT_SID = 'your_account_sid'\n",
    "TWILIO_AUTH_TOKEN = 'your_auth_token'\n",
    "TWILIO_PHONE_NUMBER = 'your_twilio_phone_number'\n",
    "ALERT_RECEIVER_PHONE = '+917988123720'  # Replace with your phone number\n",
    "\n",
    "# Initialize Twilio client\n",
    "twilio_client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)\n",
    "\n",
    "def send_sms_alert(body):\n",
    "    try:\n",
    "        message = twilio_client.messages.create(\n",
    "            body=body,\n",
    "            from_=TWILIO_PHONE_NUMBER,\n",
    "            to=ALERT_RECEIVER_PHONE\n",
    "        )\n",
    "        logging.info(f\"SMS alert sent: {body}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send SMS alert: {e}\")\n",
    "\n",
    "# Load models for face detection, age, gender classification, and gesture recognition\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "gestureModel = \"gesture_recognition_model.h5\"  # Replace with actual model if available\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "gestureNet = None  # Replace with actual gesture model loading if available\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "def faceBox(faceNet, frame):\n",
    "    frameHeight, frameWidth = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123], swapRB=False)\n",
    "    faceNet.setInput(blob)\n",
    "    detection = faceNet.forward()\n",
    "    bboxs = []\n",
    "    for i in range(detection.shape[2]):\n",
    "        confidence = detection[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            x1 = int(detection[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detection[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detection[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detection[0, 0, i, 6] * frameHeight)\n",
    "            bboxs.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    return frame, bboxs\n",
    "\n",
    "def classify_gender(face, genderNet, genderList):\n",
    "    if face.size == 0:\n",
    "        return \"Unknown\"\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPred = genderNet.forward()\n",
    "    return genderList[genderPred[0].argmax()]\n",
    "\n",
    "def detect_gesture(frame, gestureNet):\n",
    "    if gestureNet is None:\n",
    "        return \"Normal\"\n",
    "    # Placeholder for actual gesture detection\n",
    "    return \"Normal\"\n",
    "\n",
    "def is_lone_woman_detected(gender_counts):\n",
    "    current_hour = datetime.now().hour\n",
    "    if (current_hour >= 20 or current_hour <= 6) and gender_counts['Female'] == 1 and gender_counts['Male'] == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_anomalies(frame, gender_counts):\n",
    "    # Placeholder for advanced anomaly detection\n",
    "    return False\n",
    "\n",
    "# Real-time video capture\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "padding = 20\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame.\")\n",
    "            break\n",
    "\n",
    "        frame, bboxs = faceBox(faceNet, frame)\n",
    "        gender_counts = {\"Male\": 0, \"Female\": 0}\n",
    "\n",
    "        if not bboxs:\n",
    "            print(\"No faces detected.\")\n",
    "        \n",
    "        for bbox in bboxs:\n",
    "            face = frame[max(0, bbox[1] - padding):min(bbox[3] + padding, frame.shape[0] - 1),\n",
    "                         max(0, bbox[0] - padding):min(bbox[2] + padding, frame.shape[1] - 1)]\n",
    "            \n",
    "            gender = classify_gender(face, genderNet, genderList)\n",
    "            if gender in gender_counts:\n",
    "                gender_counts[gender] += 1\n",
    "\n",
    "            label = f\"{gender}\"\n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1] - 30), (bbox[2], bbox[1]), (0, 255, 0), -1)\n",
    "            cv2.putText(frame, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        if is_lone_woman_detected(gender_counts):\n",
    "            alert_message = \"ALERT: Lone Woman Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_sms_alert(alert_message)\n",
    "        \n",
    "        gesture = detect_gesture(frame, gestureNet)\n",
    "        if gesture == \"Suspicious\":\n",
    "            alert_message = \"ALERT: Suspicious Gesture Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_sms_alert(alert_message)\n",
    "\n",
    "        if detect_anomalies(frame, gender_counts):\n",
    "            alert_message = \"ALERT: Anomaly Detected!\"\n",
    "            cv2.putText(frame, alert_message, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            logging.info(alert_message)\n",
    "            send_sms_alert(alert_message)\n",
    "\n",
    "        # Show the frame in OpenCV window\n",
    "        try:\n",
    "            cv2.imshow(\"Women Safety Analytics\", frame)\n",
    "        except cv2.error as e:\n",
    "            print(f\"OpenCV error: {e}\")\n",
    "\n",
    "        # Check for 'q' key to break the loop\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    video.release()\n",
    "    try:\n",
    "        cv2.destroyAllWindows()\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV error during cleanup: {e}\")\n",
    "    print(\"Video capture released.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
